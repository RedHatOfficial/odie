
:icons: font

= Application Deployment Scripts

== Scripts Overview & Prerequisites

The scripts contained in this directory are designed to provide the necessary tooling to run the ODIE applications on OpenShift Container Platform by creating the needed OpenShift objects and containers, and deploying the applications into them. This is accomplished by three separate scripts:

- provision-project.sh
- provision-app.sh
- start-build.sh

These scripts are explained in more detail in the following sections.

=== Pre-requisites

It is assumed that when running these scripts that it is being run from a Linux host that has access to the OpenShift Masters, and a *`oc login`* as been performed:

----
$ oc login https://openshift.rhc-lab.iad.redhat.com:8443
Authentication required for https://openshift.rhc-lab.iad.redhat.com:8443 (openshift)
Username: kfranklin
Password:
Login successful.

You don't have any projects. You can try to create a new project, by running

    $ oc new-project <projectname>

Welcome to OpenShift! See 'oc help' to get started.
----

It is also assumed that an OpenShift Project has been created:

----
$ oc new-project smoketest
Now using project "smoketest" on server "https://openshift.rhc-lab.iad.redhat.com:8443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.
----

=== provision-project.sh

This script performs a provisioning on a previously created OpenShift Project by creating the needed OpenShift project level objects and resources such as the needed service-accounts and the project wide truststore. It will also provision, install, and configure a Application within the Project that will host the AMQ Broker while will run within a OpenShift pod in the project. It will also create the Application associated services, route, deployment-config, persistent-volume-claim, and runtime.

WARNING: This script should only be run once per OpenShift Project.


==== Prerequisites

This script requires that the OpenShift Project that will be supplied as a argument has already been created.

==== Arguments

----
provision-project.sh <openshift-project-name> <java-keystore-file> <java-keystore-file-password> <java-truststore-file> <java-truststore-file-password>
----

This script requires five arguments:

- *<openshift-project-name>* This is the name of the OpenShift Project in which to perform the actions.
- *<java-keystore-file>* This is the location of the Java Keystore File containing the private/public certificate entry for the AMQ server to use.
- *<java-keystore-file-password>* This is the password for the Java Keystore File specified by the *<java-keystore-file>*.
- *<java-truststore-file>* This is the location of the Java Keystore File containing the public certificates to use to verify in two-way TLS.
- *<java-truststore-file-password>* This is the password for the Java Keystore File specified by the *<java-truststore-file-file>*.

=== provision-app.sh

This script performs a creation and provisioning of an application within a previously provisioned project. This script will create the services, routes, certificate secrets, runtimes, and build configs needed for the EAP and PostgreSQL OpenShift Pods to support the application/s. This should be run once for each desired EAP/PostgreSQL Pod pair.

==== Prerequisites

This script requires that the *`provision-project.sh`* has already been run successfully.

==== Arguments

----
provision-app.sh <openshift-project-name> <openshift-application-name> <java-keystore-file> <java-keystore-file-password> <java-keystore-alias>
----

This script requires five arguments:

- *<openshift-project-name>* This is the name of the OpenShift Project in which to perform the actions.
- *<openshift-application-name>* This is the name of the application to be deployed in OpenShift, this should match the root-context of the application.
- *<java-keystore-file>* This is the location of the Java Keystore File containing the private/public certificate entry for the EAP server to use.
- *<java-keystore-file-password>* This is the password for the Java Keystore File specified by the *<java-keystore-file>*.
- *<java-keystore-alias>* This is the alias for the private/public certificate entry in the Java Keystore File specified by the *<java-keystore-file>*.

=== start-build.sh

This script performs a creation and provisioning of an application within a previously provisioned project. This script will create the services, routes, runtimes, and build configs needed for the EAP and PostgreSQL OpenShift Pods to support the application/s. This should be run once for each desired EAP/PostgreSQL Pod pair.

==== Prerequisites

This script requires that the *`provision-eap-certs.sh`* has already been run successfully.

==== Arguments

----
start-build.sh <openshift-project-name>* <openshift-application-name>* --from-build <application-build-directory>
----

This script requires two arguments:

- *<openshift-project-name>* This is the name of the OpenShift Project in which to perform the actions.
- *<openshift-application-name>* This is the name of the application to be deployed in OpenShift, this should match the root-context of the application.
- *<application-build-directory>* This is the location on the local file system that contains the application-build-directory. For further information, please consult the section of this document on the application-build-directory

== application-build-directory

The *application-build-directory* passed to the *start-build.sh* script must follow a particular directory structure convention that is dictated by the docker commands in the Dockerfile located at the base of the directory. The application-build-directory contains:

- *configuration* This directory contains the configuration files that will be placed in to */opt/eap/standalone/configuration/* directory of the EAP container, such as *standalone-openshift.xml*
- *deployments* This directory contains the deployment artifacts that will be placed in to the */deployments/* directory of the EAP container and that will be deployed.
- *modules* This directory contains any modules that will e placed in to the */opt/eap/modules/* directory of the EAP container.
- *properties* This directory contains any application specific property files that will be placed in to the */data/* directory of the EAP container.
- *Dockerfile* This Dockerfile dictates what actions will be performed when the container image is created, this file should not require changes beyond exclusions to the *COPY* commands by commenting out as needed.

== fielding_kit_reference Smoke-Test & Walk-Through

This section will walk-through a smoke-test of the OCP installation as well as act a walk-through of the scripts, as well as explain the application-build-directory via the example located within the *odie-ocp-installer/application_deployment/fielding_kit_reference/* directory of the repository. This also assume that your current working directory is the *odie-ocp-installer/application_deployment* directory of the repository.

. &#9744;Login to the OpenShift master using valid credentials with *self-provisioning* rights to create projects in OpenShift.
+
----
$ oc login https://10.15.161.146:8443
Authentication required for https://10.15.161.146:8443 (openshift)
Username: developer
Password:
Login successful.

You have one project on this server: "myproject"

Using project "myproject".
----

. &#9744;Create a new project titled *smoketest*.
+
----
$ oc new-project smoketest
Now using project "smoketest" on server "https://10.15.161.146:8443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git

to build a new example application in Ruby.
----

. &#9744;Call the *provision-project.sh* script passing in a three arguments, *<openshift-project-name>*, *<java-truststore-file>*, and *<java-truststore-file-password>*.
+
This smoke-test will be using example certificates that have been placed in the git repository under *odie-ocp-installer/application_deployment/fielding_kit_reference/example_ca*
+
The *<openshift-project-name>* argument is the name of the OpenShift Project we created in step 2, *smoketest*. The *<java-truststore-file>* is the location of the Java Keystore File containing the public certificates to use to verify in two-way TLS, we are using a test JKS, *./fielding_kit_reference/example_ca/trusts.jks*. The *<java-truststore-file-password>* is the password for the Java Keystore File specified by the *<java-truststore-file>, in our example below *ExamplePassword#2017*.
+
----
$ ./provision-project.sh smoketest ./fielding_kit_reference/example_ca/trusts.jks 'ExamplePassword#2017'

## Verify environment is ready ###########
Checking if oc command is available                                             [SUCCESS]
Checking if oc command is logged in                                             [SUCCESS]
Switching to project smoketest                                                  [SUCCESS]

## Provision Project ###########
Create Service account eap-service-account                                      [SUCCESS]
Add Policies to Service Account                                                 [SUCCESS]
Add image-puller role to Service Account                                        [SUCCESS]
Add image-puller role to Service Account                                        [SUCCESS]

## Provision AMQ Broker ###########
Create Service account amq-service-account                                      [SUCCESS]
Add Policies to Service Account                                                 [SUCCESS]
Add view to default project user                                                [SUCCESS]
Add image-puller role to Service Account                                        [SUCCESS]
testte amq runtime from template                                                [\]
secret "broker-amq" createdplate                                                [|]
service "broker-amq-amqp" created
service "broker-amq-mqtt" created                                               [/]
service "broker-amq-stomp" created
service "broker-amq-tcp" created
route "broker-ssl-amq" created
deploymentconfig "broker-amq" created
persistentvolumeclaim "broker-amq-claim" created                                [-]
Create amq runtime from template                                                [SUCCESS]

## Provision Project Truststore Secret ##########
testte project truststore secret from template                                  [\]
secret "smoketest-truststore-file" createdlate                                  [|]
secret "smoketest-truststore-password" created                                  [/]
Create project truststore secret from template                                  [SUCCESS]
----
+
If we call the *oc get pods* command, we can see that the *broker-amq-#-#####* pod has been deployed. Note that your output may or may not include the *broker-amq-#-deploy* pod:
+
----
$ oc get pods
NAME                  READY     STATUS    RESTARTS   AGE
broker-amq-1-deploy   1/1       Running   0          32s
broker-amq-1-h8d0g    1/1       Running   0          28s
----

. &#9744;Now that the project has been provisioned, call the *provision-app.sh* passing in five arguments, *<openshift-project-name>*, *<openshift-application-name>*, *<java-keystore-file>*, *<java-keystore-file-password>*, and *<java-keystore-alias>*.
+
This smoke-test will be using example certificates that have been placed in the git repository under *odie-ocp-installer/application_deployment/fielding_kit_reference/example_ca*
+
The *<openshift-project-name>* argument is the name of the OpenShift Project we created in step 2, *smoketest*. The *<openshift-application-name>* argument is the name of the application we will be deploying and also it's root context, *jboss-helloworld-mdb*. The *<java-keystore-file>* is the location of the Java Keystore File containing the private/public certificate entry for the EAP server to use, we are using a test JKS, *./fielding_kit_reference/example_ca/jboss-helloworld-mdb.lab.iad.consulting.redhat.com.jks*. The *<java-keystore-file-password>* is the password for the Java Keystore File specified by the *<java-keystore-file>, in our example below *ExamplePassword#2017*. The *<java-keystore-alias>* is the alias for the private/public certificate entry in the Java Keystore File specified by the *<java-keystore-file>, in our example below *jboss-helloworld-mdb-entry*.
+
----
$ ./provision-app.sh smoketest jboss-helloworld-mdb ./fielding_kit_reference/example_ca/jboss-helloworld-mdb.lab.iad.consulting.redhat.com.jks 'ExamplePassword#2017' jboss-helloworld-mdb-entry

## Verify environment is ready ###########
Checking if oc command is available                                             [SUCCESS]
Checking if oc command is logged in                                             [SUCCESS]

## Provision PostgreSQL Database ###########
Create db runtime from template                                                 [SUCCESS]

## Provision Application ###########
Create runtime from template                                                    [SUCCESS]
Create route from template                                                      [SUCCESS]
Create build config from template                                               [SUCCESS]
Customized environment variables for jboss-helloworld-mdb                       [SKIPPED]
Custom script for jboss-helloworld-mdb                                          [SKIPPED]
testte application keystore secret from template                                [\]
secret "jboss-helloworld-mdb-eap-keystore-file" created                         [|]
secret "jboss-helloworld-mdb-eap-keystore-strings" created
Create application keystore secret from template                                [SUCCESS]
----
+
If we call the *oc get pods* command, we can see that the *jboss-helloworld-mdb-postgresql-#-#####* pod has been deployed. Note that your output may or may not include the *jboss-helloworld-mdb-postgresql-#-deploy* pod/s, and it may take a few minutes for the pod to reach a *Running* STATUS:
+
----
$ oc get pods
NAME                                      READY     STATUS    RESTARTS   AGE
broker-amq-1-h8d0g                        1/1       Running   0          2m
jboss-helloworld-mdb-postgresql-1-0kd2v   1/1       Running   0          22s
----

. &#9744;The final step in the script process is to kick off the build. Call the *start-build.sh* passing in three arguments, *<openshift-project-name>*, *<openshift-application-name>*, *<application-build-directory>*. The *<openshift-project-name>* argument is the name of the OpenShift Project we created in step 2, *smoketest*. The *<openshift-application-name>* argument is the name of the application we will be deploying and also it's root context, that we created in step 4, *jboss-helloworld-mdb*. The *<application-build-directory>* is the location on the local file system that contains the application-build-directory, in the case of the *fielding_kit_reference* example, *fielding_kit_reference/helloworld-mdb*.
+
----
$ ./start-build.sh smoketest jboss-helloworld-mdb --from-dir ./fielding_kit_reference/helloworld-mdb/
Checking if oc command is available                                             [SUCCESS]
Checking if oc command is logged in                                             [SUCCESS]
Uploading directory "fielding_kit_reference/helloworld-mdb" as binary input for the build ...
build "jboss-helloworld-mdb-2" started                                          [\]
Receiving source from STDIN as archive ...
...
Pushed 13/13 layers, 100% complete                                              [|]
Push successfulild                                                              [/]
Start binary build                                                              [SUCCESS]
NAME                                     HOST/PORT                                                               PATH      SERVICES                              PORT      TERMINATION            WILDCARD
broker-ssl-amq                           broker-ssl-amq-smoketest.192.168.1.189.xip.io                                     broker-amq-tcp                        <all>     edge                   None
jboss-helloworld-mdb-services-internal   jboss-helloworld-mdb-services-internal-smoketest.192.168.1.189.xip.io             jboss-helloworld-mdb-services-https   <all>     passthrough/Redirect   None
Displaying available routes                                                     [SUCCESS]
NAME                                     HOST/PORT                                                               PATH      SERVICES                              PORT      TERMINATION            WILDCARD
broker-ssl-amq                           broker-ssl-amq-smoketest.192.168.1.189.xip.io                                     broker-amq-tcp                        <all>     edge                   None
jboss-helloworld-mdb-services-internal   jboss-helloworld-mdb-services-internal-smoketest.192.168.1.189.xip.io             jboss-helloworld-mdb-services-https   <all>     passthrough/Redirect   None
----

. &#9744;Ensure that the pods are up and running and in a ready state, all the *oc get pods* command, we can see that the jboss-helloworld-mdb-#-##### pod has been deployed. Note that your output may or may not include the jboss-helloworld-mdb-#-build or jboss-helloworld-mdb-#-deploy pod/s, and it may take a few minutes for the pod to reach a *Running* STATUS. Make sure before proceeding that all of the runtime pods are in a *Running* STATUS with a READY *1/1* or they are in a *Completed* STATUS per the below:
+
----
$ oc get pods
NAME                                      READY     STATUS      RESTARTS   AGE
broker-amq-1-h8d0g                        1/1       Running     0          3m
jboss-helloworld-mdb-1-7hkm3              1/1       Running     0          47s
jboss-helloworld-mdb-2-build              0/1       Completed   0          51s
jboss-helloworld-mdb-postgresql-1-0kd2v   1/1       Running     0          1m
----

. &#9744; Now that the application has been deployed and is running, it is time to test it. First get the address of the exposed route via the *oc describe* command. In this case we will perform a describe on the route named *jboss-helloworld-mdb-services-internal*
+
----
$ oc describe route/jboss-helloworld-mdb-services-internal
Name:			jboss-helloworld-mdb-services-internal
Namespace:		smoketest
Created:		13 minutes ago
Labels:			app=jboss-helloworld-mdb
			public_facing=false
			release=v1
			template=odie-public-routes
Description:		Route for odie HTTP/UI services.
Annotations:		openshift.io/host.generated=true
Requested Host:		jboss-helloworld-mdb-services-internal-smoketest.cloudapps.rhc-lab.iad.redhat.com
			  exposed on router router 13 minutes ago
Path:			/
TLS Termination:	<none>
Insecure Policy:	<none>
Endpoint Port:		<all endpoint ports>

Service:	jboss-helloworld-mdb-services-http
Weight:		100 (100%)
Endpoints:	<none>
----
+
We can see from the above that the route exposes the service *jboss-helloworld-mdb-services-http* at the *Requested Host:* field with a value of *jboss-helloworld-mdb-services-internal-smoketest.cloudapps.rhc-lab.iad.redhat.com*, this will differ in your environment, so take not of what the value of your *Requested Host:*, as the document will use $ROUTE_HOST so you must substitute as needed.
+
NOTE: The rest of the document uses the *curl* command extensively, although a web-broswer should be used just as well.

. &#9744; Using the Linux *curl* command, confirm that the following is returned:
+
----
$ curl https://$ROUTE_HOST/jboss-helloworld-mdb/ --insecure
<!--
    JBoss, Home of Professional Open Source
    Copyright 2014, Red Hat, Inc. and/or its affiliates, and individual
    contributors by the @authors tag. See the copyright.txt in the
    distribution for a full listing of individual contributors.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
-->
<!-- Plain HTML page that kicks us into the app -->

<html>
<head>
<title>Sample Application</title>
</head>
<body>
<h1>Sample Application</h1>
<ul>
  <li><a href="DatabaseTestServletClient">DatabaseTestServletClient</a></li>
  <li><a href="HelloWorldMDBServletClient">HelloWorldMDBServletClient</a></li>
</ul>
</body>
</html>
----
+
This confirms that the application as at least deployed successfully.

. &#9744; Now using the deployed web-app, we will test a basic JDBC connection. Run the following *curl* command
+
----
$ curl https://$ROUTE_HOST/jboss-helloworld-mdb/DatabaseTestServletClient --insecure
<h1>Quickstart: This example demonstrates the use of <strong>JDBC</strong> in Red Hat JBoss Enterprise Application Platform 6.</h1>
<p>Trying JNDI lookup of <strong>java:jboss/datasources/TestPostgreSQLDS</strong></p>
<p>Executing Query <strong>SELECT 1;</strong> on <strong>java:jboss/datasources/TestPostgreSQLDS</strong></p>
<p><strong>Connection worked to java:jboss/datasources/TestPostgreSQLDS</strong></p>
----
+
Note in the above that *Connection worked to java:jboss/datasources/TestPostgreSQLDS* was returned, meaning that a *SELECT 1;* query was successfully performed on the PostgreSQL database pod. This validates that the EAP pod can successfully access the PostgreSQL pod.

. &#9744; Run the *oc get pods* command.
+
----
$ oc get pods
NAME                                      READY     STATUS      RESTARTS   AGE
broker-amq-1-t9j27                        1/1       Running     0          26m
jboss-helloworld-mdb-1-6xs76              1/1       Running     4          21m
jboss-helloworld-mdb-2-build              0/1       Completed   0          22m
jboss-helloworld-mdb-postgresql-1-s7h2p   1/1       Running     0          25m
----
+
Take note of the *jboss-helloworld-mdb-#-#####* pod in a *Running* STATUS, which in our case is *jboss-helloworld-mdb-1-6xs76*. Perform a *oc logs* command against this pod to see the logs of the Pod and container/s within.
+
----
oc logs -f jboss-helloworld-mdb-1-6xs76
Service account has sufficient permissions to view pods in kubernetes (HTTP 200). Clustering will be available.
Missing SSO_URL. Unable to properly configure SSO-enabled applications
Running jboss-eap-6/eap64-openshift image, version 1.5
-XX:+UseParallelGC -XX:MinHeapFreeRatio=20 -XX:MaxHeapFreeRatio=40 -XX:GCTimeRatio=4 -XX:AdaptiveSizePolicyWeight=90 -XX:MaxMetaspaceSize=100m -XX:+ExitOnOutOfMemoryError
=========================================================================
  JBoss Bootstrap Environment
  JBOSS_HOME: /opt/eap
  JAVA: /usr/lib/jvm/java-1.8.0/bin/java
  ...
=========================================================================
Picked up JAVA_TOOL_OPTIONS: -Duser.home=/home/jboss -Duser.name=jboss
20:04:48,276 INFO  [org.jboss.modules] (main) JBoss Modules version 1.3.9.Final-redhat-1
20:04:49,029 INFO  [org.jboss.msc] (main) JBoss MSC version 1.1.7.SP1-redhat-1
...
20:04:54,877 INFO  [org.apache.activemq.ra.ActiveMQEndpointWorker] (default-threads - 1) Successfully established connection to broker [tcp://broker-amq-tcp:61616?jms.rmIdFromConnectionId=true]
20:04:54,877 INFO  [org.apache.activemq.ra.ActiveMQEndpointWorker] (default-threads - 2) Successfully established connection to broker [tcp://broker-amq-tcp:61616?jms.rmIdFromConnectionId=true]
----
+
The last two messages above from *org.apache.activemq.ra.ActiveMQEndpointWorker* indicate that EAP has successfully connected to the ActiveMQ broker

. &#9744;Using the Linux *curl* command, confirm that the following is returned:
+
----
$ curl https://$ROUTE_HOST/jboss-helloworld-mdb/HelloWorldMDBServletClient --insecure
<h1>Quickstart: This example demonstrates the use of <strong>JMS 1.1</strong> and <strong>EJB 3.1 Message-Driven Bean</strong> in Red Hat JBoss Enterprise Application Platform 6.</h1>
<p>Sending messages to <em>queue://HELLOWORLDMDBQueue</em></p>
<h2>Following messages will be send to the destination:</h2>
Message (0): This is message 1</br>
Message (1): This is message 2</br>
Message (2): This is message 3</br>
Message (3): This is message 4</br>
Message (4): This is message 5</br>
<p><i>Go to your JBoss EAP server console or log to see the result of messages processing</i></p>
----
+
This produces a series of 5 messages that are sent to a ActiveMQ queue. If we check the EAP logs again, we will not that 5 new messages have appeared from Java class *org.jboss.as.quickstarts.mdb.HelloWorldQueueMDB* informing that the messages are being processed successfully:
+
----
oc logs -f jboss-helloworld-mdb-1-6xs76
Service account has sufficient permissions to view pods in kubernetes (HTTP 200). Clustering will be available.
...
20:10:40,041 INFO  [class org.jboss.as.quickstarts.mdb.HelloWorldQueueMDB] (default-threads - 3) Received Message from queue: This is message 1
20:10:40,041 INFO  [class org.jboss.as.quickstarts.mdb.HelloWorldQueueMDB] (default-threads - 5) Received Message from queue: This is message 3
20:10:40,040 INFO  [class org.jboss.as.quickstarts.mdb.HelloWorldQueueMDB] (default-threads - 4) Received Message from queue: This is message 2
20:10:40,041 INFO  [class org.jboss.as.quickstarts.mdb.HelloWorldQueueMDB] (default-threads - 7) Received Message from queue: This is message 5
20:10:40,041 INFO  [class org.jboss.as.quickstarts.mdb.HelloWorldQueueMDB] (default-threads - 6) Received Message from queue: This is message 4
----
+
Given the above we can state that EAP can successfully send to and process from ActiveMQ queues on the Broker pod.

. &#9744;Using the Linux *curl* command, confirm that the following is returned:
+
----
$ curl http://jboss-helloworld-mdb-services-internal-smoketest.cloudapps.rhc-lab.iad.redhat.com/jboss-helloworld-mdb/HelloWorldMDBServnt?topic
<h1>Quickstart: This example demonstrates the use of <strong>JMS 1.1</strong> and <strong>EJB 3.1 Message-Driven Bean</strong> in Red Hat JBoss Enterprise Application Platform 6.</h1>
<p>Sending messages to <em>topic://HELLOWORLDMDBTopic</em></p>
<h2>Following messages will be send to the destination:</h2>
Message (0): This is message 1</br>
Message (1): This is message 2</br>
Message (2): This is message 3</br>
Message (3): This is message 4</br>
Message (4): This is message 5</br>
<p><i>Go to your JBoss EAP server console or log to see the result of messages processing</i></p>
----
+
This produces a series of 5 messages that are published to a ActiveMQ topic. If we check the EAP logs again, we will not that 5 new messages have appeared from Java class *org.jboss.as.quickstarts.mdb.HelloWorldTopicMDB* informing that the messages are being processed successfully:
+
----
oc logs -f jboss-helloworld-mdb-1-6xs76
Service account has sufficient permissions to view pods in kubernetes (HTTP 200). Clustering will be available.
...
20:16:59,313 INFO  [class org.jboss.as.quickstarts.mdb.HelloWorldTopicMDB] (default-threads - 13) Received Message from topic: This is message 1
20:16:59,314 INFO  [class org.jboss.as.quickstarts.mdb.HelloWorldTopicMDB] (default-threads - 14) Received Message from topic: This is message 2
20:16:59,319 INFO  [class org.jboss.as.quickstarts.mdb.HelloWorldTopicMDB] (default-threads - 15) Received Message from topic: This is message 3
20:16:59,322 INFO  [class org.jboss.as.quickstarts.mdb.HelloWorldTopicMDB] (default-threads - 16) Received Message from topic: This is message 4
20:16:59,323 INFO  [class org.jboss.as.quickstarts.mdb.HelloWorldTopicMDB] (default-threads - 17) Received Message from topic: This is message 5
----
+
Given the above we can state that EAP can successfully send to and process from ActiveMQ topics on the Broker pod.

. &#9744;Now clean up the smoke-test application by performing the following command:
+
----
$ oc delete project smoketest
project "smoketest" deleted
----


== build.properties

The aforementioned configuration scripts also support the ability to set the values for the command-line parameters via a *build.properties* file. An example *build.properties* file is provided in the *odie-ocp-installer/application_deployment/fielding_kit_reference/* directory that will allow you to run the smoketest having to provide any command-line parameters as input. The scripts will look for a *build.properties* file in the current working directory, and if found, will use the values contained therein. Below is a example *build.properties* file

----
$ cat fielding_kit_reference/build.properties
# Property file with needed variables for building the application

# EAP_IMAGE_NAME: OCP Docker image name to be used for EAP, this will likely not need to be changed
#   REQUIRED
EAP_IMAGE_NAME=jboss-eap-6/eap64-openshift
# AMQ_IMAGE_NAME: OCP Docker image name to be used for AMQ, this will likely not need to be changed
#   REQUIRED
AMQ_IMAGE_NAME=jboss-amq-6/amq62-openshift
# POSTGRES_IMAGE_NAME: OCP Docker image name to be used for PostgreSQL, this will likely not need to be changed
#   REQUIRED
POSTGRES_IMAGE_NAME=rhscl/postgresql-95-rhel7

EAP_OPENSHIFT_DATA_DIR=/data

# PROJECT_NAME: OCP project that should be provisioned created via the `oc new-project <PROJECT_NAME>` command
#   REQUIRED
PROJECT_NAME=smoketest
# APPLICATION_NAME: OCP application that will be created and provisioned by the scripts
#   REQUIRED
APPLICATION_NAME=helloworld-mdb

# BUILD_PATH: The application-build-directory that contains the application to be deployed. If not set, this defaults to `./${APPLICATION_NAME}/`
#   OPTIONAL
BUILD_PATH=./helloworld-mdb/
# EAP_READINESS_PROBE_PATH: The url that the OCP readiness probe should use to ensure that the application is running and ready to take requests. If this is not set, it will default to `/${APPLICATION_NAME}/`
#   OPTIONAL
EAP_READINESS_PROBE_PATH=/jboss-helloworld-mdb/

# KEYSTORE_FILE: Java Keystore File containing the private/public certificate entry for the EAP server to use
#   REQUIRED
KEYSTORE_FILE=example_ca/jboss-helloworld-mdb.lab.iad.consulting.redhat.com.jks
# KEYSTORE_PASSWORD: Password for the Java Keystore File specified by the *<java-keystore-file>*
#   REQUIRED
KEYSTORE_PASSWORD=ExamplePassword#2017
# KEYSTORE_ALIAS: Alias for the private/public certificate entry in the Java Keystore File specified by the *<java-keystore-file>*
#   REQUIRED
KEYSTORE_ALIAS=jboss-helloworld-mdb-entry
# TRUSTSTORE_FILE: Java Keystore File containing the public certificates to use to verify in two-way TLS.
#   REQUIRED
TRUSTSTORE_FILE=example_ca/trusts.jks
# TRUSTSTORE_PASSWORD: Password for the Java Keystore File specified by the *<java-truststore-file-file>*
#   REQUIRED
TRUSTSTORE_PASSWORD=ExamplePassword#2017

# EAP_ROUTE_HOST: FQDN to use for the route servicing EAP, if not set, defaults to  `broker-ssl-amq-${PROJECT_NAME}.<environment-domain-name>`
#   OPTIONAL
#EAP_ROUTE_HOST=helloworld-smoketest.cloudapps.rhc-lab.iad.redhat.com
# AMQ_ROUTE_HOST: FQDN to use for the route servicing AMQ, if not set, defaults to `${APPLICATION_NAME}-services-internal-${PROJECT_NAME}.<environment-domain-name>`
#   OPTIONAL
#AMQ_ROUTE_HOST=amq-smoketest.rhc-lab.iad.redhat.com
----

=== build.properties Settings Reference
include::build_properties.adoc[]
